---
layout:     post
title:      Lightgbm
subtitle:   
date:       2020-03-16
author:     YY
header-img: img/post-bg-rwd.jpg
catalog: true
tags:
    - 集成学习
    - 机器学习
---
  

#  Lightgbm

# 快速入门指南

本文档是 LightGBM CLI 版本的快速入门指南。

参考安装指南先安装 LightGBM 。

**其他有帮助的链接列表**

*   [参数](./Parameters.rst)
*   [参数调整](./Parameters-Tuning.rst)
*   [Python 包快速入门](./Python-Intro.rst)
*   [Python API](./Python-API.rst)

## 训练数据格式

LightGBM 支持 [CSV](https://en.wikipedia.org/wiki/Comma-separated_values), [TSV](https://en.wikipedia.org/wiki/Tab-separated_values) 和 [LibSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/) 格式的输入数据文件。

Label 是第一列的数据，文件中是不包含 header（标题） 的。

### 类别特征支持

12/5/2016 更新:

==LightGBM 可以直接使用 categorical feature（类别特征）（不需要单独编码）==。 [Expo data](http://stat-computing.org/dataexpo/2009/) 实验显示，与 one-hot 编码相比，其速度提高了 8 倍。

有关配置的详细信息，请参阅 [参数](./Parameters.rst) 章节。

### 权重和 Query/Group 数据

LightGBM 也支持加权训练，它需要一个额外的 [加权数据](./Parameters.rst#io-parameters) 。 它需要额外的 [query 数据](./Parameters.rst#io-parameters) 用于排名任务。

11/3/2016 更新:

1.  现在支持 header（标题）输入
2.  可以指定 label 列，权重列和 query/group id 列。 索引和列都支持
3.  可以指定一个被忽略的列的列表

## 参数快速查看

参数格式是 `key1=value1 key2=value2 ...` 。 参数可以在配置文件和命令行中。

一些重要的参数如下 :

*   `config`, 默认=`""`, type（类型）=string, alias（别名）=`config_file`
    *   配置文件的路径
*   `task`, 默认=`train`, type（类型）=enum, options（可选）=`train`, `predict`, `convert_model`
    *   `train`, alias（别名）=`training`, 用于训练
    *   `predict`, alias（别名）=`prediction`, `test`, 用于预测。
    *   `convert_model`, 用于将模型文件转换为 if-else 格式， 在 [转换模型参数](./Parameters.rst#convert-model-parameters) 中了解更多信息
*   `application`, 默认=`regression`, 类型=enum, 可选=`regression`, `regression_l1`, `huber`, `fair`, `poisson`, `quantile`, `quantile_l2`, `binary`, `multiclass`, `multiclassova`, `xentropy`, `xentlambda`, `lambdarank`, 别名=`objective`, `app`
    *   回归 application
        *   `regression_l2`, L2 损失, 别名=`regression`, `mean_squared_error`, `mse`
        *   `regression_l1`, L1 损失, 别名=`mean_absolute_error`, `mae`
        *   `huber`, [Huber loss](https://en.wikipedia.org/wiki/Huber_loss)
        *   `fair`, [Fair loss](https://www.kaggle.com/c/allstate-claims-severity/discussion/24520)
        *   `poisson`, [Poisson regression](https://en.wikipedia.org/wiki/Poisson_regression)
        *   `quantile`, [Quantile regression](https://en.wikipedia.org/wiki/Quantile_regression)
        *   `quantile_l2`, 与 `quantile` 类似, 但是使用 L2 损失
    *   `binary`, 二进制`log loss`_ 分类 application
    *   多类别分类 application
        *   `multiclass`, [softmax](https://en.wikipedia.org/wiki/Softmax_function) 目标函数, `num_class` 也应该被设置
        *   `multiclassova`, [One-vs-All](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest) 二元目标函数, `num_class` 也应该被设置
    *   交叉熵 application
        *   `xentropy`, 交叉熵的目标函数 (可选线性权重), 别名=`cross_entropy`
        *   `xentlambda`, 交叉熵的替代参数化, 别名=`cross_entropy_lambda`
        *   label 是在 [0, 1] 间隔中的任何东西
    *   `lambdarank`, [lambdarank](https://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf) application
        *   在 lambdarank 任务中 label 应该是 `int` 类型，而较大的数字表示较高的相关性（例如，0:bad, 1:fair, 2:good, 3:perfect）
        *   `label_gain` 可以用来设置 `int` label 的 gain(weight)（增益（权重））
*   `boosting`, 默认=`gbdt`, type=enum, 选项=`gbdt`, `rf`, `dart`, `goss`, 别名=`boost`, `boosting_type`
    *   `gbdt`, traditional Gradient Boosting Decision Tree（传统梯度提升决策树）
    *   `rf`, 随机森林
    *   `dart`, [Dropouts meet Multiple Additive Regression Trees](https://arxiv.org/abs/1505.01866)
    *   `goss`, Gradient-based One-Side Sampling（基于梯度的单面采样）
*   `data`, 默认=`""`, 类型=string, 别名=`train`, `train_data`
    *   训练数据， LightGBM 将从这个数据训练
*   `valid`, 默认=`""`, 类型=multi-string, 别名=`test`, `valid_data`, `test_data`
    *   验证/测试 数据，LightGBM 将输出这些数据的指标
    *   支持多个验证数据，使用 `,` 分开
*   `num_iterations`, 默认=`100`, 类型=int, 别名=`num_iteration`, `num_tree`, `num_trees`, `num_round`, `num_rounds`, `num_boost_round`
    *   boosting iterations/trees 的数量
*   `learning_rate`, 默认=`0.1`, 类型=double, 别名=`shrinkage_rate`
    *   shrinkage rate（收敛率）
*   `num_leaves`, 默认=`31`, 类型=int, 别名=`num_leaf`
    *   在一棵树中的叶子数量
*   `tree_learner`, 默认=`serial`, 类型=enum, 可选=`serial`, `feature`, `data`, `voting`, 别名=`tree`
    *   `serial`, 单个 machine tree 学习器
    *   `feature`, 别名=`feature_parallel`, feature parallel tree learner（特征并行树学习器）
    *   `data`, 别名=`data_parallel`, data parallel tree learner（数据并行树学习器）
    *   `voting`, 别名=`voting_parallel`, voting parallel tree learner（投票并行树学习器）
    *   参考 [Parallel Learning Guide（并行学习指南）](./Parallel-Learning-Guide.rst) 来了解更多细节
*   `num_threads`, 默认=`OpenMP_default`, 类型=int, 别名=`num_thread`, `nthread`
    *   LightGBM 的线程数
    *   为了获得最好的速度，将其设置为 **real CPU cores（真实 CPU 内核）** 数量，而不是线程数（大多数 CPU 使用 [hyper-threading](https://en.wikipedia.org/wiki/Hyper-threading) 来为每个 CPU core 生成 2 个线程）
    *   对于并行学习，不应该使用全部的 CPU cores ，因为这会导致网络性能不佳
*   `max_depth`, 默认=`-1`, 类型=int
    *   树模型最大深度的限制。 当 `#data` 很小的时候，这被用来处理 overfit（过拟合）。 树仍然通过 leaf-wise 生长
    *   `&lt; 0` 意味着没有限制
*   `min_data_in_leaf`, 默认=`20`, 类型=int, 别名=`min_data_per_leaf` , `min_data`, `min_child_samples`
    *   一个叶子中的最小数据量。可以用这个来处理过拟合。
*   `min_sum_hessian_in_leaf`, 默认=`1e-3`, 类型=double, 别名=`min_sum_hessian_per_leaf`, `min_sum_hessian`, `min_hessian`, `min_child_weight`
    *   一个叶子节点中最小的 sum hessian 。类似于 `min_data_in_leaf` ，它可以用来处理过拟合。

想要了解全部的参数， 请参阅 [Parameters（参数）](./Parameters.rst).

## 运行 LightGBM

对于 Windows:

```
lightgbm.exe config=your_config_file other_args ...

```

对于 Unix:

```
./lightgbm config=your_config_file other_args ...

```

参数既可以在配置文件中，也可以在命令行中，命令行中的参数优先于配置文件。例如下面的命令行会保留 `num_trees=10` ，并忽略配置文件中的相同参数。

```
./lightgbm config=train.conf num_trees=10

```

## 示例

*   [Binary Classification（二元分类）](https://github.com/Microsoft/LightGBM/tree/master/examples/binary_classification)
*   [Regression（回归）](https://github.com/Microsoft/LightGBM/tree/master/examples/regression)
*   [Lambdarank](https://github.com/Microsoft/LightGBM/tree/master/examples/lambdarank)
*   [Parallel Learning（并行学习）](https://github.com/Microsoft/LightGBM/tree/master/examples/parallel_learning)



---


# Python 包的相关介绍

该文档给出了有关 LightGBM Python 软件包的基本演练.

**其它有用的链接列表**

*   [Python 例子](https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide)
*   [Python API](./Python-API.rst)
*   [参数优化](./Parameters-Tuning.rst)

## 安装

安装 Python 软件包的依赖, `setuptools`, `wheel`, `numpy` 和 `scipy` 是必须的, `scikit-learn` 对于 sklearn 接口和推荐也是必须的:

```
pip install setuptools wheel numpy scipy scikit-learn -U

```

参考 [Python-package](https://github.com/Microsoft/LightGBM/tree/master/python-package) 安装指南文件夹.

为了验证是否安装成功, 可以在 Python 中 `import lightgbm` 试试:

```
import lightgbm as lgb

```

## 数据接口

LightGBM Python 模块能够使用以下几种方式来加载数据:

*   libsvm/tsv/csv txt format file（libsvm/tsv/csv 文本文件格式）
*   Numpy 2D array, pandas object（Numpy 2维数组, pandas 对象）
*   LightGBM binary file（LightGBM 二进制文件）

加载后的数据存在 `Dataset` 对象中.

**要加载 ligsvm 文本文件或 LightGBM 二进制文件到 Dataset 中:**

```
train_data = lgb.Dataset('train.svm.bin')

```

**要加载 numpy 数组到 Dataset 中:**

```
data = np.random.rand(500, 10)  # 500 个样本, 每一个包含 10 个特征
label = np.random.randint(2, size=500)  # 二元目标变量,  0 和 1
train_data = lgb.Dataset(data, label=label)

```

**要加载 scpiy.sparse.csr_matrix 数组到 Dataset 中:**

```
csr = scipy.sparse.csr_matrix((dat, (row, col)))
train_data = lgb.Dataset(csr)

```

**保存 Dataset 到 LightGBM 二进制文件将会使得加载更快速:**

```
train_data = lgb.Dataset('train.svm.txt')
train_data.save_binary('train.bin')

```

**创建验证数据:**

```
test_data = train_data.create_valid('test.svm')

```

or

```
test_data = lgb.Dataset('test.svm', reference=train_data)

```

在 LightGBM 中, 验证数据应该与训练数据一致（格式一致）.

**指定 feature names（特征名称）和 categorical features（分类特征）:**

```
train_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3'])

```

LightGBM 可以直接使用 categorical features（分类特征）作为 input（输入）. 它不需要被转换成 one-hot coding（独热编码）, 并且它比 one-hot coding（独热编码）更快（约快上 8 倍）

**注意**: 在你构造 `Dataset` 之前, 你应该将分类特征转换为 `int` 类型的值.

**当需要时可以设置权重:**

```
w = np.random.rand(500, )
train_data = lgb.Dataset(data, label=label, weight=w)

```

或者

```
train_data = lgb.Dataset(data, label=label)
w = np.random.rand(500, )
train_data.set_weight(w)

```

并且你也可以使用 `Dataset.set_init_score()` 来初始化 score（分数）, 以及使用 `Dataset.set_group()` ；来设置 group/query 数据以用于 ranking（排序）任务.

**内存的高使用:**

LightGBM 中的 `Dataset` 对象由于只需要保存 discrete bins（离散的数据块）, 因此它具有很好的内存效率. 然而, Numpy/Array/Pandas 对象的内存开销较大. 如果你关心你的内存消耗. 您可以根据以下方式来节省内存:

1.  在构造 `Dataset` 时设置 `free_raw_data=True` （默认为 `True`）
2.  在 `Dataset` 被构造完之后手动设置 `raw_data=None`
3.  调用 `gc`

## 设置参数

LightGBM 可以使用一个 pairs 的 list 或一个字典来设置 [参数](./Parameters.rst). 例如:

*   Booster（提升器）参数:

    ```
    param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}
    param['metric'] = 'auc'

    ```

*   您还可以指定多个 eval 指标:

    ```
    param['metric'] = ['auc', 'binary_logloss']

    ```

## 训练

训练一个模型时, 需要一个 parameter list（参数列表）和 data set（数据集）:

```
num_round = 10
bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])

```

在训练完成后, 可以使用如下方式来存储模型:

```
bst.save_model('model.txt')

```

训练后的模型也可以转存为 JSON 的格式:

```
json_model = bst.dump_model()

```

以保存模型也可以使用如下的方式来加载.

```
bst = lgb.Booster(model_file='model.txt')  #init model

```

## 交叉验证

使用 5-折 方式的交叉验证来进行训练（4 个训练集, 1 个测试集）:

```
num_round = 10
lgb.cv(param, train_data, num_round, nfold=5)

```

## 提前停止

如果您有一个验证集, 你可以使用提前停止找到最佳数量的 boosting rounds（梯度次数）. 提前停止需要在 `valid_sets` 中至少有一个集合. 如果有多个，它们都会被使用:

```
bst = lgb.train(param, train_data, num_round, valid_sets=valid_sets, early_stopping_rounds=10)
bst.save_model('model.txt', num_iteration=bst.best_iteration)

```

该模型将开始训练, 直到验证得分停止提高为止. 验证错误需要至少每个 &lt;cite&gt;early_stopping_rounds&lt;/cite&gt; 减少以继续训练.

如果提前停止, 模型将有 1 个额外的字段: &lt;cite&gt;bst.best_iteration&lt;/cite&gt;. 请注意 &lt;cite&gt;train()&lt;/cite&gt; 将从最后一次迭代中返回一个模型, 而不是最好的一个.

This works with both metrics to minimize (L2, log loss, etc.) and to maximize (NDCG, AUC). Note that if you specify more than one evaluation metric, all of them will be used for early stopping.

这与两个度量标准一起使用以达到最小化（L2, 对数损失, 等等）和最大化（NDCG, AUC）. 请注意, 如果您指定多个评估指标, 则它们都会用于提前停止.

## 预测

已经训练或加载的模型都可以对数据集进行预测:

```
# 7 个样本, 每一个包含 10 个特征
data = np.random.rand(7, 10)
ypred = bst.predict(data)

```

如果在训练过程中启用了提前停止, 可以用 &lt;cite&gt;bst.best_iteration&lt;/cite&gt; 从最佳迭代中获得预测结果:

```
ypred = bst.predict(data, num_iteration=bst.best_iteration)

```


---


# 参数优化

该页面包含了 LightGBM 中所有的参数.

**其他有用链接列表**

*   [参数](./Parameters.rst)
*   [Python API](./Python-API.rst)

## 针对 Leaf-wise (最佳优先) 树的参数优化

LightGBM uses the [leaf-wise](./Features.rst#leaf-wise-best-first-tree-growth) tree growth algorithm, while many other popular tools use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can convenge much faster. However, the leaf-wise growth may be over-fitting if not used with the appropriate parameters.

LightGBM 使用 [leaf-wise](./Features.rst#leaf-wise-best-first-tree-growth) 的树生长策略, 而很多其他流行的算法采用 depth-wise 的树生长策略. 与 depth-wise 的树生长策略相较, leaf-wise 算法可以收敛的更快. 但是, 如果参数选择不当的话, leaf-wise 算法有可能导致过拟合.

To get good results using a leaf-wise tree, these are some important parameters:

想要在使用 leaf-wise 算法时得到好的结果, 这里有几个重要的参数值得注意:

1.  `num_leaves`. This is the main parameter to control the complexity of the tree model. Theoretically, we can set `num_leaves = 2^(max_depth)` to convert from depth-wise tree. However, this simple conversion is not good in practice. The reason is, when number of leaves are the same, the leaf-wise tree is much deeper than depth-wise tree. As a result, it may be over-fitting. Thus, when trying to tune the `num_leaves`, we should let it be smaller than `2^(max_depth)`. For example, when the `max_depth=6` the depth-wise tree can get good accuracy, but setting `num_leaves` to `127` may cause over-fitting, and setting it to `70` or `80` may get better accuracy than depth-wise. Actually, the concept `depth` can be forgotten in leaf-wise tree, since it doesn’t have a correct mapping from `leaves` to `depth`.

1.  `num_leaves`. 这是控制树模型复杂度的主要参数. 理论上, 借鉴 depth-wise 树, 我们可以设置 `num_leaves = 2^(max_depth)` 但是, 这种简单的转化在实际应用中表现不佳. 这是因为, 当叶子数目相同时, leaf-wise 树要比 depth-wise 树深得多, 这就有可能导致过拟合. 因此, 当我们试着调整 `num_leaves` 的取值时, 应该让其小于 `2^(max_depth)`. 举个例子, 当 `max_depth=6` 时(这里译者认为例子中, 树的最大深度应为7), depth-wise 树可以达到较高的准确率.但是如果设置 `num_leaves` 为 `127` 时, 有可能会导致过拟合, 而将其设置为 `70` 或 `80` 时可能会得到比 depth-wise 树更高的准确率. 其实, `depth` 的概念在 leaf-wise 树中并没有多大作用, 因为并不存在一个从 `leaves` 到 `depth` 的合理映射.
2.  `min_data_in_leaf`. This is a very important parameter to deal with over-fitting in leaf-wise tree. Its value depends on the number of training data and `num_leaves`. Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.

1.  `min_data_in_leaf`. 这是处理 leaf-wise 树的过拟合问题中一个非常重要的参数. 它的值取决于训练数据的样本个树和 `num_leaves`. 将其设置的较大可以避免生成一个过深的树, 但有可能导致欠拟合. 实际应用中, 对于大数据集, 设置其为几百或几千就足够了.
2.  `max_depth`. You also can use `max_depth` to limit the tree depth explicitly.

1.  `max_depth`. 你也可以利用 `max_depth` 来显式地限制树的深度.

## 针对更快的训练速度

*   Use bagging by setting `bagging_fraction` and `bagging_freq`
*   Use feature sub-sampling by setting `feature_fraction`
*   Use small `max_bin`
*   Use `save_binary` to speed up data loading in future learning
*   Use parallel learning, refer to [并行学习指南](./Parallel-Learning-Guide.rst)
*   通过设置 `bagging_fraction` 和 `bagging_freq` 参数来使用 bagging 方法
*   通过设置 `feature_fraction` 参数来使用特征的子抽样
*   使用较小的 `max_bin`
*   使用 `save_binary` 在未来的学习过程对数据加载进行加速
*   使用并行学习, 可参考 [并行学习指南](./Parallel-Learning-Guide.rst)

## 针对更好的准确率

*   Use large `max_bin` (may be slower)
*   Use small `learning_rate` with large `num_iterations`
*   Use large `num_leaves` (may cause over-fitting)
*   Use bigger training data
*   Try `dart`
*   使用较大的 `max_bin` （学习速度可能变慢）
*   使用较小的 `learning_rate` 和较大的 `num_iterations`
*   使用较大的 `num_leaves` （可能导致过拟合）
*   使用更大的训练数据
*   尝试 `dart`

## 处理过拟合

*   Use small `max_bin`
*   Use small `num_leaves`
*   Use `min_data_in_leaf` and `min_sum_hessian_in_leaf`
*   Use bagging by set `bagging_fraction` and `bagging_freq`
*   Use feature sub-sampling by set `feature_fraction`
*   Use bigger training data
*   Try `lambda_l1`, `lambda_l2` and `min_gain_to_split` for regularization
*   Try `max_depth` to avoid growing deep tree
*   使用较小的 `max_bin`
*   使用较小的 `num_leaves`
*   使用 `min_data_in_leaf` 和 `min_sum_hessian_in_leaf`
*   通过设置 `bagging_fraction` 和 `bagging_freq` 来使用 bagging
*   通过设置 `feature_fraction` 来使用特征子抽样
*   使用更大的训练数据
*   使用 `lambda_l1`, `lambda_l2` 和 `min_gain_to_split` 来使用正则
*   尝试 `max_depth` 来避免生成过深的树



